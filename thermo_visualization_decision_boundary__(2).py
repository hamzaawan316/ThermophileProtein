# -*- coding: utf-8 -*-
"""Thermo_visualization_decision_boundary_ (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FuDQlW_g7VUQD006jLAdGyWXvIZwJCvZ
"""

# -*- coding: utf-8 -*-
"""Visualization decision boundary .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hl38aGqB5OGN5MpJCTq1_3NMtffTpTQM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM, Input
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from sklearn.pipeline import make_pipeline
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from matplotlib.colors import ListedColormap
from sklearn.inspection import DecisionBoundaryDisplay
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM
from tensorflow.keras.utils import to_categorical
from sklearn.neural_network import MLPClassifier
import lightgbm as lgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, GRU, Conv1D, Flatten, SimpleRNN, Input



# Load dataset
df = pd.read_csv("/content/ThemophilicProteinFV.csv")
# Separate features and target
X = df.drop(columns=['label'])
Y = df['label']

# Convert all non-numeric values to NaN and then handle them
X = X.apply(pd.to_numeric, errors='coerce')
# Option 1: Drop rows with NaN values
X = X.dropna()
# Ensure the target labels are aligned with the cleaned features
Y = Y.loc[X.index]

# Normalize the feature data
X = X.to_numpy()
Y = Y.to_numpy()
scaler = MinMaxScaler().fit(X)
X = scaler.transform(X)
X = np.nan_to_num(X.astype('float32'))

# Apply t-SNE for dimensionality reduction
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

# Split dataset (now based on t-SNE transformed features)
X_train, X_test, Y_train, Y_test = train_test_split(X_tsne, Y, test_size=0.2, random_state=42, stratify=Y)

# Define classifiers
classifiers = {
    "SVM": SVC(kernel="rbf", probability=True, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Extra Trees": ExtraTreesClassifier(n_estimators=100, random_state=42),
    "MLP": MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=500, random_state=42),
}

# CNN Model
cnn = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    Conv1D(32, kernel_size=2, activation="relu"),
    Flatten(),
    Dense(1, activation="sigmoid")
])
cnn.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
cnn.fit(X_train.reshape(-1, X_train.shape[1], 1), Y_train, epochs=10, verbose=0)

# LSTM Model
lstm = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    LSTM(32, return_sequences=False),
    Dense(1, activation="sigmoid")
])
lstm.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
lstm.fit(X_train.reshape(-1, X_train.shape[1], 1), Y_train, epochs=10, verbose=0)

# Add CNN and LSTM to classifiers dictionary
classifiers["CNN"] = cnn
classifiers["LSTM"] = lstm

# Create meshgrid for decision boundaries
h = 0.2
x_min, x_max = X_tsne[:, 0].min() - 0.5, X_tsne[:, 0].max() + 0.5
y_min, y_max = X_tsne[:, 1].min() - 0.5, X_tsne[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Define colors
cm = plt.cm.RdBu
cm_bright = ListedColormap(["#FF0000", "#0000FF"])

# Plot decision boundaries
figure = plt.figure(figsize=(17, 12))
i = 1

# Iterate over classifiers
for name, clf in classifiers.items():
    ax = plt.subplot(2, 3, i)

    # Train classifier & calculate accuracy
    if name in ["CNN", "LSTM"]:
        # Reshape for deep learning models
        X_test_reshaped = X_test.reshape(-1, X_train.shape[1], 1)
        Y_pred = (clf.predict(X_test_reshaped) > 0.5).astype(int).flatten()
        score = np.mean(Y_pred == Y_test)
        Z = (clf.predict(np.c_[xx.ravel(), yy.ravel()].reshape(-1, X_train.shape[1], 1)) > 0.5).astype(int).flatten()
    else:
        clf.fit(X_train, Y_train)
        score = clf.score(X_test, Y_test)
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

    # Reshape and plot decision boundary
    Z = Z.reshape(xx.shape)
    contour = ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)

    # Add legend for decision boundary classes
    handles = [
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor="red", markersize=10, label="0"),
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor="blue", markersize=10, label="1")
    ]
    ax.legend(handles=handles, loc="upper left")

    # Scatter plot for train/test points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cm_bright, edgecolors="black", s=25, label="Train Data")
    ax.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap=cm_bright, alpha=0.6, edgecolors="black", s=25, label="Test Data")

    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(name)

    # Display accuracy score
    ax.text(xx.max() - 0.3, yy.min() + 0.2, f"{score:.2f}".lstrip("0"), size=15, horizontalalignment="right")

    i += 1

figure.subplots_adjust(left=0.02, right=0.98)
plt.show()

# Define classifiers
classifiers = {
    "SVM": SVC(kernel="rbf", probability=True, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Extra Trees": ExtraTreesClassifier(n_estimators=100, random_state=42),
    "MLP": MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=500, random_state=42),
}

# CNN Model
cnn = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    Conv1D(32, kernel_size=2, activation="relu"),
    Flatten(),
    Dense(1, activation="sigmoid")
])
cnn.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
cnn.fit(X_train.reshape(-1, X_train.shape[1], 1), Y_train, epochs=10, verbose=0)

# LSTM Model
lstm = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    LSTM(32, return_sequences=False),
    Dense(1, activation="sigmoid")
])
lstm.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
lstm.fit(X_train.reshape(-1, X_train.shape[1], 1), Y_train, epochs=10, verbose=0)

# Add CNN and LSTM to classifiers dictionary
classifiers["CNN"] = cnn
classifiers["LSTM"] = lstm

# Create meshgrid for decision boundaries
h = 0.2
x_min, x_max = X_tsne[:, 0].min() - 0.5, X_tsne[:, 0].max() + 0.5
y_min, y_max = X_tsne[:, 1].min() - 0.5, X_tsne[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Define new colors
cm = plt.cm.GnBu   # Background colormap (Green-Blue shades)
cm_bright = ListedColormap(["#ADD8E6", "#008000"])  # Light Blue, Green

# Plot decision boundaries
figure = plt.figure(figsize=(17, 12))
i = 1

# Iterate over classifiers
for name, clf in classifiers.items():
    ax = plt.subplot(2, 3, i)

    # Train classifier & calculate accuracy
    if name in ["CNN", "LSTM"]:
        # Reshape for deep learning models
        X_test_reshaped = X_test.reshape(-1, X_train.shape[1], 1)
        Y_pred = (clf.predict(X_test_reshaped) > 0.5).astype(int).flatten()
        score = np.mean(Y_pred == Y_test)
        Z = (clf.predict(np.c_[xx.ravel(), yy.ravel()].reshape(-1, X_train.shape[1], 1)) > 0.5).astype(int).flatten()
    else:
        clf.fit(X_train, Y_train)
        score = clf.score(X_test, Y_test)
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

    # Reshape and plot decision boundary
    Z = Z.reshape(xx.shape)
    contour = ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)

    # Updated legend colors
    handles = [
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor="#ADD8E6", markersize=10, label="0"),  # Light Blue
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor="#008000", markersize=10, label="1")   # Green
    ]
    ax.legend(handles=handles, loc="upper left")

    # Scatter plot with new cmap
    ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cm_bright, edgecolors="black", s=25, label="Train Data")
    ax.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap=cm_bright, alpha=0.6, edgecolors="black", s=25, label="Test Data")

    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(name)

    # Display accuracy score
    ax.text(xx.max() - 0.3, yy.min() + 0.2, f"{score:.2f}".lstrip("0"), size=15, horizontalalignment="right")

    i += 1

figure.subplots_adjust(left=0.02, right=0.98)
plt.show()

# 6) Classifiers (train on 2D t-SNE features)
classifiers = {
    "SVM": SVC(kernel="rbf", probability=True, random_state=RANDOM_STATE),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),
    "Extra Trees": ExtraTreesClassifier(n_estimators=100, random_state=RANDOM_STATE),
    "MLP": MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=500, random_state=RANDOM_STATE),
}

# 7) Simple CNN & LSTM models that take (2,1) shaped inputs (t-SNE dims=2)
# Note: these are toy models because input is only 2 dims (t-SNE). Use small epochs for demo.
def make_cnn(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        Conv1D(16, kernel_size=1, activation="relu"),
        Flatten(),
        Dense(1, activation="sigmoid")
    ])
    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
    return model

def make_lstm(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(16, return_sequences=False),
        Dense(1, activation="sigmoid")
    ])
    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
    return model

# Prepare deep-model training data (reshape to [n_samples, 2, 1])
X_train_seq = X_train_tsne.reshape(-1, X_train_tsne.shape[1], 1)
X_test_seq  = X_test_tsne.reshape(-1, X_test_tsne.shape[1], 1)

cnn = make_cnn((X_train_tsne.shape[1], 1))
lstm = make_lstm((X_train_tsne.shape[1], 1))

# Train shallow epochs to keep runtime moderate. Increase if you want better fit.
cnn.fit(X_train_seq, y_train, epochs=8, verbose=0)
lstm.fit(X_train_seq, y_train, epochs=8, verbose=0)

# Add to dictionary (we'll treat differently during prediction)
classifiers["CNN"] = cnn
classifiers["LSTM"] = lstm

# 8) Color scheme requested by user (background + points)
# Background colors: light green, light blue
cm_background = ListedColormap(["#b3ffb3", "#b3d9ff"])  # light green / light blue

# Point colors (class 0, class 1) strong green and strong blue to match provided image
cm_points = ListedColormap(["#00cc00", "#0066ff"])  # class 0 -> green, class 1 -> blue
edgecolor_for_points = "black"

# 9) Build a meshgrid over t-SNE space for background decision visualization
h = 0.8  # grid step (coarser because t-SNE coords can be wide)
x_min, x_max = X_tsne[:,0].min() - 5, X_tsne[:,0].max() + 5
y_min, y_max = X_tsne[:,1].min() - 5, X_tsne[:,1].max() + 5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
grid_points = np.c_[xx.ravel(), yy.ravel()]

# 10) Plot predictions (one subplot per model)
plt.figure(figsize=(17, 12))
i = 1
for name, clf in classifiers.items():
    ax = plt.subplot(2, 3, i)

    # For deep models (Keras), use .predict on reshaped grid
    if name in ["CNN", "LSTM"]:
        # Predict on test seq to compute accuracy
        y_test_pred = (clf.predict(X_test_seq) > 0.5).astype(int).flatten()
        score = accuracy_score(y_test, y_test_pred)

        # Predict on entire grid (reshape to [n_grid, 2, 1])
        grid_seq = grid_points.reshape(-1, X_train_tsne.shape[1], 1)
        grid_pred_proba = clf.predict(grid_seq, verbose=0).flatten()
        # Binarize with 0.5 threshold
        grid_pred = (grid_pred_proba > 0.5).astype(int)
    else:
        # Fit classical classifier on tsne train
        clf.fit(X_train_tsne, y_train)
        score = clf.score(X_test_tsne, y_test)
        # For background use predict_proba if available; else predict
        try:
            grid_pred_proba = clf.predict_proba(grid_points)[:, 1]
        except Exception:
            # fallback: decision_function or predict
            try:
                df_val = clf.decision_function(grid_points)
                grid_pred_proba = (df_val - df_val.min()) / (df_val.max() - df_val.min() + 1e-9)
            except Exception:
                grid_pred_proba = clf.predict(grid_points)
        # Binarize at 0.5 (works even if grid_pred_proba is 0/1)
        grid_pred = (grid_pred_proba > 0.5).astype(int)

    # Reshape grid predictions to meshgrid for background plotting
    Z = grid_pred.reshape(xx.shape)

    # Plot background: use ListedColormap light colors
    ax.contourf(xx, yy, Z, alpha=0.45, cmap=cm_background, levels=[-0.5, 0.5, 1.5])

    # Plot the predicted class per sample as faint background points (optional)
    # We'll overlay the true train/test with strong point colors
    # Convert full dataset X_tsne predictions for overlay if desired (compute using same model)
    if name in ["CNN", "LSTM"]:
        X_full_seq = X_tsne.reshape(-1, X_tsne.shape[1], 1)
        full_pred = (clf.predict(X_full_seq, verbose=0).flatten() > 0.5).astype(int)
    else:
        full_pred = clf.predict(X_tsne)

    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=full_pred, cmap=cm_background, alpha=0.05, s=10)

    # Overlay strong colored train and test points (truth labels)
    ax.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=y_train,
               cmap=cm_points, edgecolors=edgecolor_for_points, s=40, label="Train")
    ax.scatter(X_test_tsne[:, 0], X_test_tsne[:, 1], c=y_test,
               cmap=cm_points, edgecolors=edgecolor_for_points, s=60, marker="^", label="Test")

    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(f"{name}   (Acc={score:.2f})")
    ax.legend(loc="upper left")
    i += 1

plt.tight_layout()
plt.show()

# 11) Optional: a clean t-SNE scatter of the *true* classes with the same point colors (like your example)
plt.figure(figsize=(10, 6))
plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y, cmap=cm_points, s=50, edgecolors='black', alpha=0.85)
plt.title("t-SNE Visualization of the Dataset (True labels)")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.gca().set_facecolor('#eaffea')  # subtle light green background for axes area
plt.show()

# =============================
# Define classifiers
# =============================
classifiers = {
    "SVM": SVC(kernel="rbf", probability=True, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Extra Trees": ExtraTreesClassifier(n_estimators=100, random_state=42),
    "MLP": MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=500, random_state=42),
}

# =============================
# Deep learning models
# =============================
# CNN model
cnn = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    Conv1D(32, kernel_size=2, activation="relu"),
    Flatten(),
    Dense(1, activation="sigmoid")
])
cnn.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
cnn.fit(X_train.reshape(-1, X_train.shape[1], 1), Y_train, epochs=10, verbose=0)

# LSTM model
lstm = Sequential([
    Input(shape=(X_train.shape[1], 1)),
    LSTM(32, return_sequences=False),
    Dense(1, activation="sigmoid")
])
lstm.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
lstm.fit(X_train.reshape(-1, X_train.shape[1], 1), Y_train, epochs=10, verbose=0)

# Add CNN & LSTM to dictionary
classifiers["CNN"] = cnn
classifiers["LSTM"] = lstm

# =============================
# Visualization
# =============================
cm = plt.cm.viridis
cm_bright = ListedColormap(["#FF7F0E", "#1F77B4"])  # orange & blue

figure = plt.figure(figsize=(17, 12))
i = 1

for name, clf in classifiers.items():
    ax = plt.subplot(2, 3, i)

    if name in ["CNN", "LSTM"]:
        # Predictions for test data
        X_test_seq = X_test.reshape(-1, X_train.shape[1], 1)
        Y_pred = (clf.predict(X_test_seq) > 0.5).astype(int).flatten()
        score = np.mean(Y_pred == Y_test)

        # Predictions for all data (for plotting in t-SNE space)
        preds = (clf.predict(X.reshape(-1, X_train.shape[1], 1)) > 0.5).astype(int).flatten()
    else:
        clf.fit(X_train, Y_train)
        score = clf.score(X_test, Y_test)

        # Predictions for all data (for plotting in t-SNE space)
        preds = clf.predict(X)

    # Plot predictions in t-SNE space
    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=preds, cmap=cm, alpha=0.6)

    # Overlay train/test data points
    ax.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1],
               c=Y_train, cmap=cm_bright,
               edgecolors="black", s=25, label="Train")
    ax.scatter(X_test_tsne[:, 0], X_test_tsne[:, 1],
               c=Y_test, cmap=cm_bright,
               edgecolors="black", s=25, marker="^", label="Test")

    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(f"{name} (Acc={score:.2f})")
    ax.legend(loc="upper left")
    i += 1

plt.tight_layout()
plt.show()





# Split dataset (now based on t-SNE transformed features)
X_train, X_test, Y_train, Y_test = train_test_split(X_tsne, Y, test_size=0.2, random_state=42, stratify=Y)

# Train SVM model
svm = SVC(kernel='rbf', random_state=42)
svm.fit(X_train_scaled, Y_train)
svm_preds = svm.predict(X_test_scaled)
print("SVM Accuracy:", accuracy_score(Y_test, svm_preds))

# Train Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, Y_train)
rf_preds = rf.predict(X_test)
print("RF Accuracy:", accuracy_score(Y_test, rf_preds))

# Train Extra Trees
et = ExtraTreesClassifier(n_estimators=100, random_state=42)
et.fit(X_train, Y_train)
et_preds = et.predict(X_test)
print("ET Accuracy:", accuracy_score(Y_test, et_preds))

# Convert labels for deep learning models
Y_train_categorical = to_categorical(Y_train)
Y_test_categorical = to_categorical(Y_test)
X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)

# CNN Model
cnn = Sequential([
    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(Y_train_categorical.shape[1], activation='sigmoid')
])
cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
cnn.fit(X_train_reshaped, Y_train_categorical, epochs=10, batch_size=32, verbose=1)

# LSTM Model
lstm = Sequential([
    LSTM(50, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
    Dense(64, activation='relu'),
    Dense(Y_train_categorical.shape[1], activation='sigmoid')
])
lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
lstm.fit(X_train_reshaped, Y_train_categorical, epochs=10, batch_size=32, verbose=1)

# t-SNE visualization
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=Y, palette="viridis", alpha=0.7)
plt.title("t-SNE Visualization of the Dataset")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.legend(title="Class")
plt.show()